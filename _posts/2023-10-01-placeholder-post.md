---
layout: post
title: "Humans hallucinate too"
date: 2023-10-01
categories: Blog
tags: [AI, GitHub Copilot, Claude, code-generation, thoughts]
---

## "Do you use AI to generate code?"

The question caught me off guard during a recent conversation. Not because I don't use AI—I do, and quite extensively. Maybe because I hadn't really articulated my thoughts about it, my immediate reaction surprised even myself. Instead of praising tools like GitHub Copilot or Claude, which have become indispensable in my workflow, I found myself dwelling on their limitations.

"Well, AI hallucinates," I heard myself say, "and you often end up just being a code reviewer." At the time, it felt like a reasonable answer. The conversation moved on, and I didn't think much of it. But a day later, reflecting on what I'd said, I realized how incomplete that response had been. Yes, these are real challenges, but they're hardly the whole story.

## The Instinctive Response

I knew the reality was more nuanced. Maybe it was a reaction to the constant AI hype. When everyone around me seems sold on AI's "limitless potential", don't I have some responsibility to highlight its limitations? But in doing so, was I misrepresenting my own experience?

The truth is, I spoke before I'd really thought it through. It wasn't until later that I realized my offhand response painted AI in a particularly harsh light. This despite the fact that I use these tools daily, trust their output more than I did six months ago (with verification), and appreciate how they accelerate my work.

## Speaking Before Thinking

The truth is, I spoke before I had time to collect my thoughts. It wasn't until a day later, while reflecting on the conversation, that I realized how my instinctive response had painted AI tools in a particularly harsh light. This was despite my actual experience with them being quite positive. I use them daily, trust their output more than I did 6 months ago (with verification), and appreciate their ability to accelerate my work.

## The Ironic Parallel

And then it hit me: wasn't this exactly what I had criticized AI for?  

AI "hallucinates." It produces convincing output that doesn't quite align with reality. Yet here I was, a human, doing exactly the same thing—offering a confident answer that didn't fully reflect what I actually thought. I had, in effect, *hallucinated*.  

## A Path Forward

My takeaway: I need to write more. Not to share knowledge, but to organize my thoughts *before* I'm put on the spot. Maybe by building my own library of well-reasoned perspectives — my own "chains of thought" — I can reduce these moments of human hallucination where my mouth outruns my mind.

If I'm going to hold AI to this standard, shouldn't I do the same for myself?
